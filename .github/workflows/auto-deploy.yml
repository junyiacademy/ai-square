name: Auto Deploy

on:
  push:
    branches:
      - main        # Deploy to production
      - staging     # Deploy to staging
  workflow_dispatch:

env:
  PROJECT_ID: ai-square-463013
  REGION: asia-east1
  NODE_OPTIONS: "--max-old-space-size=4096"

jobs:
  # Check what files have changed
  detect-changes:
    name: Detect Changed Files
    runs-on: ubuntu-latest
    outputs:
      docs-only: ${{ steps.changes.outputs.docs-only }}
      has-code-changes: ${{ steps.changes.outputs.has-code-changes }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: Detect file changes
        id: changes
        run: |
          echo "üîç Analyzing changed files..."

          # Get list of changed files
          if [ "${{ github.event_name }}" == "push" ]; then
            CHANGED_FILES=$(git diff --name-only HEAD^ HEAD)
          else
            CHANGED_FILES=$(git diff --name-only origin/main..HEAD)
          fi

          echo "Changed files:"
          echo "$CHANGED_FILES"

          # Check if only docs/markdown files changed
          DOCS_PATTERN="^(docs/|README|CHANGELOG|\.md$|\.txt$|LICENSE)"
          CODE_CHANGED=false

          while IFS= read -r file; do
            if [ -n "$file" ]; then
              # Use multiple separate checks instead of complex regex
              if ! echo "$file" | grep -qE "^docs/" && \
                 ! echo "$file" | grep -qE "README" && \
                 ! echo "$file" | grep -qE "CHANGELOG" && \
                 ! echo "$file" | grep -qE "\.md$" && \
                 ! echo "$file" | grep -qE "\.txt$" && \
                 ! echo "$file" | grep -qE "LICENSE"; then
                CODE_CHANGED=true
                echo "üîÑ Non-docs file detected: $file"
                break
              else
                echo "üìÑ Documentation file: $file"
              fi
            fi
          done <<< "$CHANGED_FILES"

          if [ "$CODE_CHANGED" = false ] && [ -n "$CHANGED_FILES" ]; then
            echo "docs-only=true" >> $GITHUB_OUTPUT
            echo "has-code-changes=false" >> $GITHUB_OUTPUT
            echo "‚úÖ Only documentation changes detected - skipping full CI"
          else
            echo "docs-only=false" >> $GITHUB_OUTPUT
            echo "has-code-changes=true" >> $GITHUB_OUTPUT
            echo "üîÑ Code changes detected - running full CI"
          fi

  # Quality checks first
  quality-checks:
    name: Code Quality & Tests
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.docs-only == 'false'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm ci --prefer-offline

      - name: Parallel Type Check & Linting
        working-directory: ./frontend
        run: |
          echo "üîÑ Running TypeScript and ESLint in parallel..."
          npx tsc --noEmit &
          TSC_PID=$!
          npx next lint &
          ESLINT_PID=$!

          # Wait for both processes
          wait $TSC_PID
          TSC_EXIT=$?
          wait $ESLINT_PID
          ESLINT_EXIT=$?

          # Check if both succeeded
          if [ $TSC_EXIT -ne 0 ] || [ $ESLINT_EXIT -ne 0 ]; then
            echo "‚ùå Type checking or linting failed"
            exit 1
          fi

          echo "‚úÖ Type checking and linting completed successfully"

      - name: Run Unit Tests
        working-directory: ./frontend
        run: npx jest --ci --coverage --testPathIgnorePatterns='/tests/integration/|/e2e/|/frontend/e2e/|\.spec\.ts$' --watchAll=false

  # Schema validation (parallel execution, doesn't block deployment)
  schema-validation:
    name: Schema Validation
    runs-on: ubuntu-latest

    permissions:
      contents: read
      id-token: write

    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: ai_square_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        working-directory: ./frontend
        run: npm ci --prefer-offline

      - name: Validate Schema Consistency
        working-directory: ./frontend
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/ai_square_db
          DB_HOST: localhost
          DB_PORT: 5432
          DB_USER: postgres
          DB_PASSWORD: postgres
          DB_NAME: ai_square_db
        run: |
          echo "üîç Checking schema consistency..."
          npx prisma migrate deploy
          npx prisma generate
          npx tsx scripts/validate-schema-consistency.ts || echo "‚ö†Ô∏è Schema validation script not found"

      - name: Check Prisma Migration Status
        working-directory: ./frontend
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/ai_square_db
        run: |
          echo "üìä Checking migration status..."
          npx prisma migrate status || true

      - name: Upload schema report on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: schema-validation-report-${{ github.run_id }}
          path: |
            frontend/prisma/schema.prisma
            frontend/prisma/migrations/

  # Deploy KSA to CDN (parallel with main deployment)
  deploy-ksa-cdn:
    name: Deploy KSA to CDN
    runs-on: ubuntu-latest
    needs: [detect-changes, quality-checks]
    if: needs.detect-changes.outputs.docs-only == 'false' && needs.quality-checks.result == 'success'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Deploy KSA to CDN
        run: |
          echo "üì¶ Deploying KSA data to CDN (parallel with main deployment)..."

          # Convert YAML to JSON
          cd frontend
          npm install --prefer-offline js-yaml
          cd public/rubrics_data/ksa_codes

          for file in *.yaml; do
            if [ "$file" != "_ksa_codes_template.yaml" ]; then
              echo "Converting $file..."
              node -e "
                const yaml = require('../../../node_modules/js-yaml');
                const fs = require('fs');
                const content = yaml.load(fs.readFileSync('$file', 'utf8'));
                fs.writeFileSync('$file'.replace('.yaml', '.json'), JSON.stringify(content));
              "
            fi
          done

          # Create bucket if not exists
          gsutil ls -b gs://ai-square-static &>/dev/null || gsutil mb -l asia-east1 gs://ai-square-static

          # Upload to Cloud Storage with parallel transfers
          gsutil -m -h "Cache-Control:public, max-age=86400" \
            cp *.json gs://ai-square-static/ksa/

          # Set public access
          gsutil iam ch allUsers:objectViewer gs://ai-square-static

          echo "‚úÖ KSA deployed to CDN: https://storage.googleapis.com/ai-square-static/ksa/"

  # Deploy job
  deploy:
    name: Deploy to Cloud Run
    runs-on: ubuntu-latest
    needs: [detect-changes, quality-checks]
    if: needs.detect-changes.outputs.docs-only == 'false' && needs.quality-checks.result == 'success'
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Determine environment
        id: env
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "environment=production" >> $GITHUB_OUTPUT
            echo "service=ai-square-production" >> $GITHUB_OUTPUT
            echo "service_account=ai-square-production@ai-square-463013.iam.gserviceaccount.com" >> $GITHUB_OUTPUT
            echo "db_instance=ai-square-db-production" >> $GITHUB_OUTPUT
            echo "üì¶ Deploying to PRODUCTION"
          else
            echo "environment=staging" >> $GITHUB_OUTPUT
            echo "service=ai-square-staging" >> $GITHUB_OUTPUT
            echo "service_account=ai-square-staging@ai-square-463013.iam.gserviceaccount.com" >> $GITHUB_OUTPUT
            echo "db_instance=ai-square-db-staging-asia" >> $GITHUB_OUTPUT
            echo "üì¶ Deploying to STAGING"
          fi

      - name: Setup Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Service Account Permissions
        run: |
          echo "üîß Configuring Service Account permissions..."

          # Vertex AI permissions
          gcloud projects add-iam-policy-binding ${{ env.PROJECT_ID }} \
            --member="serviceAccount:${{ steps.env.outputs.service_account }}" \
            --role="roles/aiplatform.user" \
            --condition=None 2>/dev/null || true

          # Cloud SQL permissions
          gcloud projects add-iam-policy-binding ${{ env.PROJECT_ID }} \
            --member="serviceAccount:${{ steps.env.outputs.service_account }}" \
            --role="roles/cloudsql.client" \
            --condition=None 2>/dev/null || true

          echo "‚úÖ Service Account permissions configured"

      - name: Run Database Migrations
        run: |
          echo "üóÑÔ∏è Running database migrations..."

          # Download Cloud SQL Proxy
          wget -q https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
          chmod +x cloud_sql_proxy
          ./cloud_sql_proxy -instances=${{ env.PROJECT_ID }}:${{ env.REGION }}:${{ steps.env.outputs.db_instance }}=tcp:5433 &
          PROXY_PID=$!

          # Wait for proxy
          sleep 5

          # Setup Node.js for Prisma
          cd frontend
          npm ci --prefer-offline --quiet

          # Run migrations based on environment
          if [[ "${{ steps.env.outputs.environment }}" == "production" ]]; then
            export DATABASE_URL="postgresql://postgres:${{ secrets.PROD_DB_PASSWORD }}@localhost:5433/ai_square_db"
          else
            export DATABASE_URL="postgresql://postgres:${{ secrets.STAGING_DB_PASSWORD }}@localhost:5433/ai_square_db"
          fi

          npx prisma migrate deploy

          # Kill proxy
          kill $PROXY_PID || true
          cd ..

          echo "‚úÖ Migrations completed"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure Docker
        run: gcloud auth configure-docker gcr.io

      - name: Build and Push Docker Image with Cache
        uses: docker/build-push-action@v6
        with:
          context: ./frontend
          file: ./frontend/${{ steps.env.outputs.environment == 'production' && 'Dockerfile.production' || 'Dockerfile.staging' }}
          push: true
          tags: |
            gcr.io/${{ env.PROJECT_ID }}/${{ steps.env.outputs.environment == 'production' && 'ai-square-production' || 'ai-square-staging' }}:${{ github.sha }}
            gcr.io/${{ env.PROJECT_ID }}/${{ steps.env.outputs.environment == 'production' && 'ai-square-production' || 'ai-square-staging' }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Deploy to Cloud Run
        run: |
          echo "üöÄ Deploying to Cloud Run with Vertex AI support..."

          if [[ "${{ steps.env.outputs.environment }}" == "production" ]]; then
            SERVICE_NAME="ai-square-production"
            IMAGE_TAG="ai-square-production"
            DB_INSTANCE="ai-square-db-production"
            DB_PASSWORD="${{ secrets.PROD_DB_PASSWORD }}"
            NEXTAUTH_URL="https://ai-square-production-731209836128.asia-east1.run.app"
          else
            SERVICE_NAME="ai-square-staging"
            IMAGE_TAG="ai-square-staging"
            DB_INSTANCE="ai-square-db-staging-asia"
            DB_PASSWORD="${{ secrets.STAGING_DB_PASSWORD }}"
            NEXTAUTH_URL="https://ai-square-staging-731209836128.asia-east1.run.app"
          fi

          # Create env vars file with proper escaping - ÂÆåÂÖ®Ë§áË£Ω deploy-staging.yml.disabled ÁöÑÊñπÂºè
          cat > /tmp/env-vars.yaml << 'EOF'
          NODE_ENV: "${{ steps.env.outputs.environment }}"
          DATABASE_URL: "postgresql://postgres:${{ steps.env.outputs.environment == 'production' && secrets.PROD_DB_PASSWORD || secrets.STAGING_DB_PASSWORD }}@/ai_square_db?host=/cloudsql/ai-square-463013:asia-east1:${{ steps.env.outputs.environment == 'production' && 'ai-square-db-production' || 'ai-square-db-staging-asia' }}"
          DB_HOST: "/cloudsql/ai-square-463013:asia-east1:${{ steps.env.outputs.environment == 'production' && 'ai-square-db-production' || 'ai-square-db-staging-asia' }}"
          DB_NAME: "ai_square_db"
          DB_USER: "postgres"
          DB_PASSWORD: "${{ steps.env.outputs.environment == 'production' && secrets.PROD_DB_PASSWORD || secrets.STAGING_DB_PASSWORD }}"
          NEXTAUTH_SECRET: "${{ secrets.NEXTAUTH_SECRET }}"
          NEXTAUTH_URL: "${{ steps.env.outputs.environment == 'production' && 'https://ai-square-production-731209836128.asia-east1.run.app' || 'https://ai-square-staging-731209836128.asia-east1.run.app' }}"
          GOOGLE_CLOUD_PROJECT: "ai-square-463013"
          GCP_PROJECT_ID: "ai-square-463013"
          VERTEX_AI_LOCATION: "us-central1"
          VERTEX_AI_PROJECT: "ai-square-463013"
          VERTEX_AI_MODEL: "gemini-2.5-flash"
          VERTEX_AI_SERVICE_ACCOUNT_JSON: '${{ secrets.VERTEX_AI_KEY }}'
          REDIS_ENABLED: "false"
          # SMTP / Email
          SMTP_HOST: "smtp.gmail.com"
          SMTP_PORT: "587"
          SMTP_USER: "${{ secrets.SMTP_USER }}"
          SMTP_PASS: "${{ secrets.SMTP_PASS }}"
          SMTP_FROM: "AI Square <ai-square@junyiacademy.org>"
          APP_BASE_URL: "${{ steps.env.outputs.environment == 'production' && 'https://ai-square-production-731209836128.asia-east1.run.app' || 'https://ai-square-staging-731209836128.asia-east1.run.app' }}"
          EOF

          # Deploy using env vars file - ÂÆåÂÖ®Ë§áË£Ω deploy-staging.yml.disabled ÁöÑÂëΩ‰ª§
          gcloud run deploy ${SERVICE_NAME} \
            --image gcr.io/${{ env.PROJECT_ID }}/${IMAGE_TAG}:${{ github.sha }} \
            --region ${{ env.REGION }} \
            --platform managed \
            --allow-unauthenticated \
            --timeout 60 \
            --memory 512Mi \
            --cpu 1 \
            --port 3000 \
            --service-account=${{ steps.env.outputs.service_account }} \
            --add-cloudsql-instances ai-square-463013:asia-east1:${DB_INSTANCE} \
            --env-vars-file=/tmp/env-vars.yaml

          # Clean up
          rm -f /tmp/env-vars.yaml

          echo "‚úÖ Deployment complete!"

      - name: Verify Deployment
        run: |
          SERVICE_URL=$(gcloud run services describe ${{ steps.env.outputs.service }} --region ${{ env.REGION }} --format 'value(status.url)')
          echo "üìç Service URL: $SERVICE_URL"

          # Wait for service to be ready
          echo "‚è≥ Waiting for service to be ready..."
          sleep 15

          # Health check
          if curl -sf "$SERVICE_URL/api/health" > /dev/null; then
            echo "‚úÖ Service is healthy!"
          else
            echo "‚ö†Ô∏è Health check failed (service may still be starting)"
          fi

          echo "‚úÖ Deployment to ${{ steps.env.outputs.environment }} complete!"
          echo "üåê Service: ${{ steps.env.outputs.service }}"
          echo "üîó URL: $SERVICE_URL"

      - name: Clean up old Cloud Run revisions
        if: success()
        run: |
          echo "üßπ Cleaning up old Cloud Run revisions..."

          SERVICE_NAME=${{ steps.env.outputs.service }}
          # Development phase - minimal retention
          if [[ "${{ steps.env.outputs.environment }}" == "production" ]]; then
            KEEP_COUNT=2  # Production: keep 2 for rollback
          else
            KEEP_COUNT=1  # Staging: keep only current
          fi

          # Get all revision names sorted by creation time (newest first)
          REVISIONS=$(gcloud run revisions list \
            --service=${SERVICE_NAME} \
            --region=${{ env.REGION }} \
            --sort-by="~metadata.creationTimestamp" \
            --format="value(metadata.name)" 2>/dev/null)

          # Count total revisions
          TOTAL=$(echo "$REVISIONS" | wc -l)

          if [ $TOTAL -gt $KEEP_COUNT ]; then
            echo "üìä Found $TOTAL revisions, keeping latest $KEEP_COUNT"

            # Get revisions to delete (skip the first KEEP_COUNT)
            TO_DELETE=$(echo "$REVISIONS" | tail -n +$((KEEP_COUNT + 1)))

            echo "$TO_DELETE" | while read -r REVISION; do
              if [ -n "$REVISION" ]; then
                echo "  üóëÔ∏è Deleting revision: $REVISION"
                gcloud run revisions delete $REVISION \
                  --region=${{ env.REGION }} \
                  --quiet 2>/dev/null || echo "    ‚ö†Ô∏è Could not delete $REVISION (might be in use)"
              fi
            done

            echo "‚úÖ Cloud Run revision cleanup complete"
          else
            echo "‚úÖ Only $TOTAL revisions exist, no cleanup needed"
          fi

      - name: Clean up old GCR images
        if: success()
        run: |
          echo "üßπ Cleaning up old container images..."

          # Function to keep only latest N images
          cleanup_repository() {
            local REPO=$1
            local KEEP_COUNT=${2:-3}  # Keep latest 3 images by default

            echo "üì¶ Cleaning $REPO (keeping latest $KEEP_COUNT images)..."

            # Get all images except the latest KEEP_COUNT
            IMAGES=$(gcloud container images list-tags "$REPO" \
              --format="get(digest)" \
              --sort-by="~timestamp" 2>/dev/null | tail -n +$((KEEP_COUNT + 1)))

            if [ -n "$IMAGES" ]; then
              echo "$IMAGES" | while read -r DIGEST; do
                if [ -n "$DIGEST" ]; then
                  echo "  üóëÔ∏è Deleting: ${DIGEST:0:12}..."
                  gcloud container images delete "$REPO@$DIGEST" --quiet --force-delete-tags 2>/dev/null || true
                fi
              done
            else
              echo "  ‚úÖ No old images to clean"
            fi
          }

          # Clean up based on environment (development phase - minimal retention)
          if [[ "${{ steps.env.outputs.environment }}" == "production" ]]; then
            cleanup_repository "gcr.io/${{ env.PROJECT_ID }}/ai-square-production" 2
          else
            cleanup_repository "gcr.io/${{ env.PROJECT_ID }}/ai-square-staging" 1
          fi

          echo "‚úÖ GCR cleanup complete"
# Trigger rebuild 2025Âπ¥ 9Êúà11Êó• ÈÄ±Âõõ 15ÊôÇ57ÂàÜ35Áßí CST
